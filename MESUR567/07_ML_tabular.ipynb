{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfcac756",
   "metadata": {},
   "source": [
    "## Machine Learning Techniques for Tabular Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e3d30",
   "metadata": {},
   "source": [
    "#### Requirements:\n",
    "- We're using the same social media dataset as last time [LINK](https://www.kaggle.com/datasets/adilshamim8/social-media-addiction-vs-relationships)\n",
    "    - I reccomend putting the .csv file into a folder called `Data` to keep things organized\n",
    "- Scikit-learn supplies great machine learning tools, if not already installed: `pip install scikit-learn`\n",
    "- Other requirements: `pandas`, `numpy`, `matplotlib`, `seaborn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3064e",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017620dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/Student_Social_Media.csv')\n",
    "df.head()\n",
    "\n",
    "# Dataset information\n",
    "print(\"\\nDataset information:\")\n",
    "df.info()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics of numerical variables:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6505f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def quick_data_viz(df, figsize=(14, 10), max_categories_box=10):\n",
    "    \"\"\"\n",
    "    Minimalistic exploratory visualization.\n",
    "    \n",
    "    - Numeric (continuous-like): histogram+KDE and boxplot\n",
    "    - Numeric (discrete/ordinal with few unique values): histogram+KDE and barplot instead of boxplot\n",
    "    - Categorical: countplot\n",
    "    - Correlation heatmap\n",
    "    - Pairwise scatter (numeric only, light)\n",
    "    \"\"\"\n",
    "    # Identify column types\n",
    "    numeric_cols = df.select_dtypes(include=['int64','float64']).columns.drop('Student_ID', errors='ignore')\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Split numeric into continuous vs discrete-ordinal\n",
    "    continuous_cols = [c for c in numeric_cols if df[c].nunique() > max_categories_box]\n",
    "    discrete_cols   = [c for c in numeric_cols if df[c].nunique() <= max_categories_box]\n",
    "    \n",
    "    # === Continuous variables ===\n",
    "    if continuous_cols:\n",
    "        fig, axes = plt.subplots(len(continuous_cols), 2, figsize=(figsize[0], 3*len(continuous_cols)))\n",
    "        axes = np.array(axes).reshape(len(continuous_cols), 2)\n",
    "        for i, col in enumerate(continuous_cols):\n",
    "            sns.histplot(df[col], kde=True, ax=axes[i,0], color='steelblue')\n",
    "            axes[i,0].set_title(f\"{col} distribution\", fontsize=10)\n",
    "            axes[i,0].set_xlabel(\"\")\n",
    "            \n",
    "            sns.boxplot(x=df[col], ax=axes[i,1], color='lightgrey')\n",
    "            axes[i,1].set_title(f\"{col} boxplot\", fontsize=10)\n",
    "            axes[i,1].set_xlabel(\"\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # === Discrete numeric variables ===\n",
    "    if discrete_cols:\n",
    "        fig, axes = plt.subplots(len(discrete_cols), 2, figsize=(figsize[0], 3*len(discrete_cols)))\n",
    "        axes = np.array(axes).reshape(len(discrete_cols), 2)\n",
    "        for i, col in enumerate(discrete_cols):\n",
    "            sns.histplot(df[col], kde=False, ax=axes[i,0], color='steelblue', discrete=True)\n",
    "            axes[i,0].set_title(f\"{col} distribution\", fontsize=10)\n",
    "            axes[i,0].set_xlabel(\"\")\n",
    "            \n",
    "            sns.countplot(x=df[col], ax=axes[i,1], color='lightgrey', order=sorted(df[col].unique()))\n",
    "            axes[i,1].set_title(f\"{col} counts\", fontsize=10)\n",
    "            axes[i,1].set_xlabel(\"\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # === Categorical variables ===\n",
    "    if len(categorical_cols) > 0:\n",
    "        ncols = 3\n",
    "        nrows = (len(categorical_cols) + ncols - 1) // ncols\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(figsize[0], nrows*3))\n",
    "        axes = axes.flatten()\n",
    "        for i, col in enumerate(categorical_cols):\n",
    "            if col == 'Country': continue\n",
    "            sns.countplot(y=df[col], ax=axes[i], color='steelblue', order=df[col].value_counts().index)\n",
    "            axes[i].set_title(f\"{col}\", fontsize=10)\n",
    "        for j in range(i+1, len(axes)):\n",
    "            axes[j].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa0d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_data_viz(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Preprocessing ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# --- Models ---\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# --- Evaluation ---\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# --- Set plot style ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef6907",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Raw data is rarely ready for modeling. We need to preprocess it. This involves:\n",
    "\n",
    "1.  **Cleaning the data**: Luckily the data we have is pretty clean (no NaN, no huge outliers, no mixed datatypes)\n",
    "1.  **Separating features and target**.\n",
    "2.  **Handling categorical variables**: ML models only understand numbers.\n",
    "3.  **Scaling numerical features**: Ensures that features with larger scales don't dominate the model.\n",
    "\n",
    "\n",
    "We will use `scikit-learn`'s `ColumnTransformer` to create a clean, reusable preprocessing pipeline. This is best practice as it prevents data leakage (information from the test set leaking into the training set) and makes the workflow reproducible.\n",
    "\n",
    "We'll perform two main transformations:\n",
    "\n",
    "  * **One-Hot Encoding**: For nominal categorical features (where order doesn't matter) like `Gender` or `Country`. It creates a new binary column for each category.\n",
    "  * **Ordinal Encoding**: For ordinal categorical features (where order matters) like `Academic_Level`. It converts categories into integer values (e.g., Undergraduate=0, Graduate=1, PhD=2).\n",
    "  * **Standard Scaling**: For numerical features. It transforms the data to have a mean of 0 and a standard deviation of 1. The formula is:\n",
    "    $$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "    Where $x$ is the original value, $\\mu$ is the mean, and $\\sigma$ is the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd919d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define features (X) and target (y)\n",
    "# We drop Student_ID as it's an identifier and not a predictive feature.\n",
    "X = df.drop(['Student_ID', 'Addicted_Score'], axis=1)\n",
    "y = df['Addicted_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18dac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Identify different column types\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "nominal_categorical_features = ['Gender', 'Country', 'Most_Used_Platform', 'Relationship_Status']\n",
    "ordinal_categorical_features = ['Academic_Level', 'Affects_Academic_Performance']\n",
    "\n",
    "# Define the order for ordinal features\n",
    "academic_levels = ['High School', 'Undergraduate', 'Graduate', 'PhD']\n",
    "academic_impact = ['No', 'Maybe', 'Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87eb12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create the preprocessing pipelines for each data type\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "# For nominal features, handle unknown categories that might appear in test data\n",
    "nominal_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "ordinal_transformer = OrdinalEncoder(categories=[academic_levels, academic_impact])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Use ColumnTransformer to apply different transformations to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('nom', nominal_transformer, nominal_categorical_features),\n",
    "        ('ord', ordinal_transformer, ordinal_categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep other columns (if any), though we've handled all\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a242220",
   "metadata": {},
   "source": [
    "Many times, we will use cross validation to get the most out of the data. Today, we'll just do a single test/train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433991e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ac8ac3",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Now we'll train three different regression models to see which performs best on our data.\n",
    "\n",
    "1.  **Linear Regression**: A simple baseline model.\n",
    "2.  **Random Forest**: A powerful and popular ensemble model.\n",
    "3.  **XGBoost**: A highly efficient gradient-boosting model, often a top performer in competitions.\n",
    "\n",
    "See last week's lecture for more detail on the pros and cons of the above models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model 1: Linear Regression ---\n",
    "# A simple model that finds the best linear relationship between features and target.\n",
    "# Equation: y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ\n",
    "\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "print(\"Linear Regression model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aeb0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model 2: Random Forest ---\n",
    "# An ensemble model that builds many decision trees and averages their predictions.\n",
    "# This helps reduce overfitting and improves accuracy.\n",
    "\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "print(\"Random Forest model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c297cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model 3: XGBoost ---\n",
    "# A gradient boosting model that builds trees sequentially, with each new tree\n",
    "# correcting the errors of the previous one. Highly effective.\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', XGBRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "print(\"XGBoost model trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669bc06a",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Training is done, but how good are our models? We need to evaluate their performance on the **test set**—data the model has never seen before.\n",
    "\n",
    "For regression, we'll use two common metrics:\n",
    "\n",
    "  * **Mean Squared Error (MSE)**: The average of the squared differences between the predicted and actual values. It penalizes larger errors more heavily.\n",
    "    $$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "  * **R-squared ($R^2$)**: The proportion of the variance in the target variable that is predictable from the features. An $R^2$ of 1 is a perfect prediction.\n",
    "    $$R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596276a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": lr_pipeline,\n",
    "    \"Random Forest\": rf_pipeline,\n",
    "    \"XGBoost\": xgb_pipeline\n",
    "}\n",
    "\n",
    "for name, pipeline in models.items():\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = {'MSE': mse, 'R2': r2}\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"  MSE: {mse:.2f}\")\n",
    "    print(f\"  R-squared: {r2:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ba52c0",
   "metadata": {},
   "source": [
    "### Visualizing the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ba2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model (XGBoost) to get predictions on the test set\n",
    "y_pred_xgb = xgb_pipeline.predict(X_test)\n",
    "\n",
    "# Create a scatter plot of actual vs. predicted values\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=y_test, y=y_pred_xgb, alpha=0.7, label='Model Predictions')\n",
    "\n",
    "# Add a line for perfect predictions (y=x)\n",
    "max_val = max(y_test.max(), y_pred_xgb.max())\n",
    "min_val = min(y_test.min(), y_pred_xgb.min())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Actual vs. Predicted Addiction Score (XGBoost)', fontsize=16)\n",
    "plt.xlabel('Actual Score', fontsize=12)\n",
    "plt.ylabel('Predicted Score', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013d19a8",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use one of the models to predict the addiction score of the student below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836785b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_student_data = {\n",
    "    'Age': 21,\n",
    "    'Gender': 'Female',\n",
    "    'Academic_Level': 'Graduate',\n",
    "    'Country': 'USA',\n",
    "    'Avg_Daily_Usage_Hours': 6.5,\n",
    "    'Most_Used_Platform': 'TikTok',\n",
    "    'Affects_Academic_Performance': 'Yes',\n",
    "    'Sleep_Hours_Per_Night': 5.0,\n",
    "    'Mental_Health_Score': 3,\n",
    "    'Relationship_Status': 'Single',\n",
    "    'Conflicts_Over_Social_Media': 7\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff6e05",
   "metadata": {},
   "source": [
    "### Predicting non-continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877fa8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "X_class = df.drop(['Student_ID', 'Addicted_Score', 'Affects_Academic_Performance', 'Sleep_Hours_Per_Night'], axis=1)\n",
    "lb = LabelBinarizer()\n",
    "y_class = lb.fit_transform(df['Affects_Academic_Performance'])\n",
    "\n",
    "\n",
    "numerical_features_class = X_class.select_dtypes(include=np.number).columns.tolist()\n",
    "nominal_categorical_features_class = ['Gender', 'Country', 'Most_Used_Platform', 'Relationship_Status']\n",
    "ordinal_categorical_features_class = ['Academic_Level']\n",
    "academic_levels = ['High School', 'Undergraduate', 'Graduate', 'PhD']\n",
    "\n",
    "preprocessor_class = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features_class),\n",
    "        ('nom', OneHotEncoder(handle_unknown='ignore'), nominal_categorical_features_class),\n",
    "        ('ord', OrdinalEncoder(categories=[academic_levels]), ordinal_categorical_features_class)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_class, y_class, test_size=0.2, random_state=42, stratify=y_class)\n",
    "\n",
    "rf_class_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_class),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "xgb_class_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_class),\n",
    "    ('classifier', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss'))\n",
    "])\n",
    "\n",
    "print(\"Training Classification Models...\")\n",
    "rf_class_pipeline.fit(X_train_c, y_train_c)\n",
    "xgb_class_pipeline.fit(X_train_c, y_train_c)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce49810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Random Forest Classifier\n",
    "y_pred_rf = rf_class_pipeline.predict(X_test_c)\n",
    "print(\"--- Random Forest Classifier Results ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_c, y_pred_rf):.2f}\")\n",
    "print(classification_report(y_test_c, y_pred_rf))\n",
    "\n",
    "# Evaluate the XGBoost Classifier\n",
    "y_pred_xgb = xgb_class_pipeline.predict(X_test_c)\n",
    "print(\"\\n--- XGBoost Classifier Results ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_c, y_pred_xgb):.2f}\")\n",
    "print(classification_report(y_test_c, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f79eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quick look at SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84619cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "X_test_transformed = xgb_class_pipeline.named_steps['preprocessor'].transform(X_test_c)\n",
    "model = xgb_class_pipeline.named_steps['classifier']\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer(X_test_transformed)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test_transformed, feature_names=xgb_class_pipeline.named_steps['preprocessor'].get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, max_display=15)\n",
    "shap.plots.waterfall(shap_values[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
